{"cells":[{"cell_type":"markdown","source":["# Ingesting and Parsing Streaming JSON from Azure Event Hubs into Microsoft Fabric Lakehouse\n","\n","In this notebook, we will learn how to ingest and parse streaming JSON from Azure Event Hubs into a Microsoft Fabric Lakehouse using the power of Spark Structured Streaming."],"metadata":{},"id":"cb639e22-a5f1-4914-be8c-aea4be0bc9b6"},{"cell_type":"markdown","source":["**Prerequisite**: \n","\n","- Setup an Azure Event Hubs namespace with an Event Hub.\n","- Setup a stream from Event Hubs Data Explorer or from your local machine to send the sample JSON below. Instructions can be found [here](#)."],"metadata":{},"id":"0762cf07-c99b-4a6b-9342-769d86ee4e56"},{"cell_type":"markdown","source":["- **Setup the connection string**"],"metadata":{},"id":"a4fa5540-6959-4670-bde3-1db40f0fefe7"},{"cell_type":"code","source":["connectionString = \"Endpoint=sb://<EVENT_HUB_NAMESPACE>.servicebus.windows.net/;SharedAccessKeyName=<SHARED_ACCESS_KEY_NAME>;SharedAccessKey=<SHARED_ACCESS_KEY>;EntityPath=<EVENT_HUB_NAME>\"\n","ehConf = {}\n","ehConf['eventhubs.connectionString'] = spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c16ccd1a-5dd9-4036-ad85-4e3d515afe3f"},{"cell_type":"markdown","source":["> Note: As a security best practice, it is recommended to keep your shared access key in Azure Key Vault. Use [Credentials utilities](https://learn.microsoft.com/en-ca/fabric/data-engineering/notebook-utilities#credentials-utilities) to access Azure Key Vault secrets in a Fabric notebook."],"metadata":{},"id":"059521c3-735a-401c-97b0-cdf05ed0d853"},{"cell_type":"markdown","source":["- **Import necessary libraries**"],"metadata":{},"id":"17bfeb3e-3180-42ef-b50f-0ed517714e5c"},{"cell_type":"code","source":["import pyspark.sql.functions as f\n","from pyspark.sql.functions import col, explode, expr, first\n","from pyspark.sql.types import *"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f08bef4f-7ca8-483d-a27f-486e221f8d03"},{"cell_type":"markdown","source":["- **Read stream from Event Hubs in a DataFrame**\n","- **Select and cast the body column as a string**\n","- **Process the JSON from the DataFrame**\n","- **Start streaming query**"],"metadata":{},"id":"85026dcc-514d-4da7-b0e1-28655a850713"},{"cell_type":"code","source":["# Read stream from Event Hubs in a DataFrame\n","df = spark \\\n","  .readStream \\\n","  .format(\"eventhubs\") \\\n","  .options(**ehConf) \\\n","  .load()\n","\n","# Select and cast the body column as a string\n","raw_data = df.selectExpr(\"CAST(body AS STRING) as message\")\n","\n","# Process the JSON from the DataFrame\n","def process_json(df, epoch_id):\n","    messages = df.collect()\n","    for message in messages:\n","        raw_message = message[\"message\"]\n","        json_df = spark.read.json(spark.sparkContext.parallelize([raw_message]))\n","        \n","        # Add a unique ID column using uuid\n","        json_df = json_df.withColumn(\"id\", expr(\"uuid()\"))\n","        \n","        # Explode the JSON column\n","        exploded_df = json_df.withColumn(\"json_col\", explode(col(\"columns\")))\n","        \n","        # Select the necessary columns dynamically\n","        columns = exploded_df.select(\"json_col.*\").columns\n","        selected_df = exploded_df.select(\"id\", \"json_col.*\")\n","        \n","        # Aggregate the columns to combine fields into a single row\n","        aggregated_df = selected_df.groupBy(\"id\").agg(\n","            *[first(c, ignorenulls=True).alias(c) for c in columns]\n","        )\n","        \n","        # Cast all columns to string\n","        final_df = aggregated_df.select([col(c).cast(StringType()).alias(c) for c in aggregated_df.columns if c != \"id\"])\n","        \n","        # Write to Delta table\n","        final_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"events\")\n","\n","#Start streaming query\n","query = raw_data \\\n","  .writeStream \\\n","  .foreachBatch(process_json) \\\n","  .option(\"checkpointLocation\", \"Files/checkpoint\") \\\n","  .trigger(processingTime='5 seconds') \\\n","  .start()\n","\n","# Await termination\n","query.awaitTermination()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3d5f14c8-e654-4d3d-b617-43dc9cf261e4"},{"cell_type":"markdown","source":["<mark>Don't forget to terminate the session.</mark>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"14c9dd19-bcf2-4116-9b71-8ee6276bbd9f"}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}}},"nbformat":4,"nbformat_minor":5}